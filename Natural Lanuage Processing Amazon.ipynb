{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package wordnet to /home/syed/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /home/syed/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           reviewerID        asin        reviewerName  helpful  helpful_num  \\\n0      A3NMPMELAZC8ZY  097293751X               Jakell  [3, 3]            3   \n1      A3O4ATU0ENBKTU  097293751X                 MAPN  [1, 1]            1   \n2      A2SYNL4YX73KNY  097293751X  R. Davidson \"Jrdpa\"  [2, 2]            2   \n3      A2Q2A6JKY95RTP  097293751X          R. Garrelts  [2, 2]            2   \n4      A21I33AWNOWMK8  9729375011               EmilyS  [1, 2]            1   \n...               ...         ...                  ...     ...          ...   \n56945  A34T0JYVRU1M2B  B00L13XFIE                Nukke  [1, 1]            1   \n56946  A1AFNMTDISXUJE  B00L13XFIE                Qutie  [1, 2]            1   \n56947  A1ZS6UQ9RVUX97  B00L13XFIE          R. Nafziger  [1, 2]            1   \n56948   AG4E44KM93P4L  B00L13XFIE             Silofish  [0, 1]            0   \n56949  A3CIIOMK18CHXM  B00L13XFIE              Viviana  [1, 1]            1   \n\n       helpful_den                                         reviewText  \\\n0                3  This book is perfect!  I'm a first time new mo...   \n1                1  I use this so that our babysitter (grandma) ca...   \n2                2  I like this log, but think it would work bette...   \n3                2  My wife and I have a six month old baby boy an...   \n4                2  I have used this book since my son was born.  ...   \n...            ...                                                ...   \n56945            1  These are really absorbent and super soft, but...   \n56946            2  I've used a variety of cloth covers and insert...   \n56947            2  The Best Bottom system is really great, especi...   \n56948            1  I am new to cloth diapering. I was leery that ...   \n56949            1  These are great. I should have bought hemp ins...   \n\n       overall                                        summary  unixReviewTime  \\\n0            5                             Great for newborns      1359244800   \n1            5  Compact and Easy way to record the milestones      1361836800   \n2            3                          Needs clearer AM & PM      1369008000   \n3            3          Expensive and Somewhat Limited Format      1381968000   \n4            5                                 Great product!      1364256000   \n...        ...                                            ...             ...   \n56945        4                                  serging issue      1385942400   \n56946        5              My favorite inserts - hemp/cotton      1365033600   \n56947        5                Convenient snap, very absorbent      1375747200   \n56948        4                                  Not too bulky      1343606400   \n56949        5                               Really absorbent      1355788800   \n\n        reviewTime  exclamationcount  questioncount  charcount  wordcount  \\\n0      01 27, 2013                 1              0        250         46   \n1      02 26, 2013                 0              0        734        148   \n2      05 20, 2013                 0              0        288         59   \n3      10 17, 2013                 0              0       2959        505   \n4      03 26, 2013                 0              0        595        117   \n...            ...               ...            ...        ...        ...   \n56945   12 2, 2013                 1              0        398         70   \n56946   04 4, 2013                 2              0       1142        216   \n56947   08 6, 2013                 0              0        504         98   \n56948  07 30, 2012                 0              0        308         61   \n56949  12 18, 2012                 0              0        128         22   \n\n       capcount  avgrating  diffrating  ishelpful  \n0             0   4.000000    1.000000          1  \n1             0   4.000000    1.000000          1  \n2             2   4.000000    1.000000          1  \n3             9   4.000000    1.000000          1  \n4             0   4.500000    0.500000          0  \n...         ...        ...         ...        ...  \n56945         0   4.142857    0.142857          1  \n56946         0   4.142857    0.857143          0  \n56947         0   4.142857    0.857143          0  \n56948         0   4.142857    0.142857          0  \n56949         0   4.142857    0.857143          1  \n\n[56950 rows x 19 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewerID</th>\n      <th>asin</th>\n      <th>reviewerName</th>\n      <th>helpful</th>\n      <th>helpful_num</th>\n      <th>helpful_den</th>\n      <th>reviewText</th>\n      <th>overall</th>\n      <th>summary</th>\n      <th>unixReviewTime</th>\n      <th>reviewTime</th>\n      <th>exclamationcount</th>\n      <th>questioncount</th>\n      <th>charcount</th>\n      <th>wordcount</th>\n      <th>capcount</th>\n      <th>avgrating</th>\n      <th>diffrating</th>\n      <th>ishelpful</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A3NMPMELAZC8ZY</td>\n      <td>097293751X</td>\n      <td>Jakell</td>\n      <td>[3, 3]</td>\n      <td>3</td>\n      <td>3</td>\n      <td>This book is perfect!  I'm a first time new mo...</td>\n      <td>5</td>\n      <td>Great for newborns</td>\n      <td>1359244800</td>\n      <td>01 27, 2013</td>\n      <td>1</td>\n      <td>0</td>\n      <td>250</td>\n      <td>46</td>\n      <td>0</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A3O4ATU0ENBKTU</td>\n      <td>097293751X</td>\n      <td>MAPN</td>\n      <td>[1, 1]</td>\n      <td>1</td>\n      <td>1</td>\n      <td>I use this so that our babysitter (grandma) ca...</td>\n      <td>5</td>\n      <td>Compact and Easy way to record the milestones</td>\n      <td>1361836800</td>\n      <td>02 26, 2013</td>\n      <td>0</td>\n      <td>0</td>\n      <td>734</td>\n      <td>148</td>\n      <td>0</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A2SYNL4YX73KNY</td>\n      <td>097293751X</td>\n      <td>R. Davidson \"Jrdpa\"</td>\n      <td>[2, 2]</td>\n      <td>2</td>\n      <td>2</td>\n      <td>I like this log, but think it would work bette...</td>\n      <td>3</td>\n      <td>Needs clearer AM &amp; PM</td>\n      <td>1369008000</td>\n      <td>05 20, 2013</td>\n      <td>0</td>\n      <td>0</td>\n      <td>288</td>\n      <td>59</td>\n      <td>2</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A2Q2A6JKY95RTP</td>\n      <td>097293751X</td>\n      <td>R. Garrelts</td>\n      <td>[2, 2]</td>\n      <td>2</td>\n      <td>2</td>\n      <td>My wife and I have a six month old baby boy an...</td>\n      <td>3</td>\n      <td>Expensive and Somewhat Limited Format</td>\n      <td>1381968000</td>\n      <td>10 17, 2013</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2959</td>\n      <td>505</td>\n      <td>9</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A21I33AWNOWMK8</td>\n      <td>9729375011</td>\n      <td>EmilyS</td>\n      <td>[1, 2]</td>\n      <td>1</td>\n      <td>2</td>\n      <td>I have used this book since my son was born.  ...</td>\n      <td>5</td>\n      <td>Great product!</td>\n      <td>1364256000</td>\n      <td>03 26, 2013</td>\n      <td>0</td>\n      <td>0</td>\n      <td>595</td>\n      <td>117</td>\n      <td>0</td>\n      <td>4.500000</td>\n      <td>0.500000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>56945</th>\n      <td>A34T0JYVRU1M2B</td>\n      <td>B00L13XFIE</td>\n      <td>Nukke</td>\n      <td>[1, 1]</td>\n      <td>1</td>\n      <td>1</td>\n      <td>These are really absorbent and super soft, but...</td>\n      <td>4</td>\n      <td>serging issue</td>\n      <td>1385942400</td>\n      <td>12 2, 2013</td>\n      <td>1</td>\n      <td>0</td>\n      <td>398</td>\n      <td>70</td>\n      <td>0</td>\n      <td>4.142857</td>\n      <td>0.142857</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>56946</th>\n      <td>A1AFNMTDISXUJE</td>\n      <td>B00L13XFIE</td>\n      <td>Qutie</td>\n      <td>[1, 2]</td>\n      <td>1</td>\n      <td>2</td>\n      <td>I've used a variety of cloth covers and insert...</td>\n      <td>5</td>\n      <td>My favorite inserts - hemp/cotton</td>\n      <td>1365033600</td>\n      <td>04 4, 2013</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1142</td>\n      <td>216</td>\n      <td>0</td>\n      <td>4.142857</td>\n      <td>0.857143</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>56947</th>\n      <td>A1ZS6UQ9RVUX97</td>\n      <td>B00L13XFIE</td>\n      <td>R. Nafziger</td>\n      <td>[1, 2]</td>\n      <td>1</td>\n      <td>2</td>\n      <td>The Best Bottom system is really great, especi...</td>\n      <td>5</td>\n      <td>Convenient snap, very absorbent</td>\n      <td>1375747200</td>\n      <td>08 6, 2013</td>\n      <td>0</td>\n      <td>0</td>\n      <td>504</td>\n      <td>98</td>\n      <td>0</td>\n      <td>4.142857</td>\n      <td>0.857143</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>56948</th>\n      <td>AG4E44KM93P4L</td>\n      <td>B00L13XFIE</td>\n      <td>Silofish</td>\n      <td>[0, 1]</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I am new to cloth diapering. I was leery that ...</td>\n      <td>4</td>\n      <td>Not too bulky</td>\n      <td>1343606400</td>\n      <td>07 30, 2012</td>\n      <td>0</td>\n      <td>0</td>\n      <td>308</td>\n      <td>61</td>\n      <td>0</td>\n      <td>4.142857</td>\n      <td>0.142857</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>56949</th>\n      <td>A3CIIOMK18CHXM</td>\n      <td>B00L13XFIE</td>\n      <td>Viviana</td>\n      <td>[1, 1]</td>\n      <td>1</td>\n      <td>1</td>\n      <td>These are great. I should have bought hemp ins...</td>\n      <td>5</td>\n      <td>Really absorbent</td>\n      <td>1355788800</td>\n      <td>12 18, 2012</td>\n      <td>0</td>\n      <td>0</td>\n      <td>128</td>\n      <td>22</td>\n      <td>0</td>\n      <td>4.142857</td>\n      <td>0.857143</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>56950 rows × 19 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "amazon_data=pd.read_csv('reviews_Baby_5_final_dataset.csv')\n",
    "amazon_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_data=amazon_data[['reviewText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=list(amazon_data['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=reviews[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_word_tokens=[]\n",
    "for review in reviews:\n",
    "    tokenized_word=word_tokenize(review)\n",
    "    document_word_tokens.append(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['this', 'book', 'be', 'perfect', 'im', 'a', 'first', 'time', 'new', 'mom', 'and', 'this', 'book', 'make', 'it', 'so', 'easy', 'to', 'keep', 'track', 'of', 'feed', 'diaper', 'change', 'sleep', 'definitely', 'would', 'recommend', 'this', 'for', 'new', 'moms', 'plus', 'its', 'small', 'enough', 'that', 'i', 'throw', 'in', 'the', 'diaper', 'back', 'for', 'doctor', 'visit']\n"
    }
   ],
   "source": [
    "print(document_word_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['i', 'like', 'this', 'log', 'but', 'think', 'it', 'would', 'work', 'better', 'with', 'clearer', 'be', 'pm', 'section', 'each', 'page', 'be', '12', 'hours', 'so', 'you', 'really', 'need', 'two', 'page', 'a', 'day', 'if', 'your', 'baby', 'feed', 'or', 'wet', 'a', 'lot', 'in', 'the', 'early', 'morning', 'hours', 'between', 'midnight', 'and', '7am', 'be', 'cram', 'those', 'in', 'to', 'the', '2', 'blank', 'space', 'above', '7am', 'right', 'now']\n"
    }
   ],
   "source": [
    "print(document_word_tokens[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('this', 3),\n ('book', 2),\n ('new', 2),\n ('diaper', 2),\n ('for', 2),\n ('be', 1),\n ('perfect', 1),\n ('im', 1),\n ('a', 1),\n ('first', 1),\n ('time', 1),\n ('mom', 1),\n ('and', 1),\n ('make', 1),\n ('it', 1)]"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist(document_word_tokens[0])\n",
    "fdist.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,string,unicodedata\n",
    "import inflect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(reviews):\n",
    "    reviews_list=[]\n",
    "    for review in reviews:\n",
    "        new_word = review.lower()\n",
    "        reviews_list.append(new_word)\n",
    "    return reviews_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(reviews):\n",
    "    reviews_list=[]\n",
    "    for review in reviews:\n",
    "        new_word=re.sub('[^\\w\\s]','',review)\n",
    "        if new_word != '':\n",
    "            reviews_list.append(new_word)\n",
    "    return reviews_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numbers(reviews):\n",
    "    reviews_list=[]\n",
    "    inf_engine=inflect.engine()\n",
    "    for review in reviews:\n",
    "        final_word_list=[]\n",
    "        words=review.split()\n",
    "        for word in words:\n",
    "            if word.isdigit():\n",
    "                final_word_list.append(inf_engine.number_to_words(word))\n",
    "            else:\n",
    "                final_word_list.append(word)\n",
    "        reviews_list.append(\" \".join(final_word_list))\n",
    "    return reviews_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_verbs(reviews):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    reviews_list=[]\n",
    "    for review in reviews:\n",
    "        final_word_list=[]\n",
    "        words=review.split()\n",
    "        for word in words:\n",
    "            final_word_list.append(lemmatizer.lemmatize(word,pos='v'))\n",
    "        reviews_list.append(\" \".join(final_word_list))\n",
    "    return reviews_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(reviews):\n",
    "    reviews=to_lowercase(reviews)\n",
    "    reviews=remove_punctuation(reviews)\n",
    "    reviews=replace_numbers(reviews)\n",
    "    reviews=lemmatize_verbs(reviews)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'this book be perfect im a first time new mom and this book make it so easy to keep track of feed diaper change sleep definitely would recommend this for new moms plus its small enough that i throw in the diaper back for doctor visit'"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'this book be perfect im a first time new mom and this book make it so easy to keep track of feed diaper change sleep definitely would recommend this for new moms plus its small enough that i throw in the diaper back for doctor visit'"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "reviews=to_lowercase(reviews)\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'this book be perfect im a first time new mom and this book make it so easy to keep track of feed diaper change sleep definitely would recommend this for new moms plus its small enough that i throw in the diaper back for doctor visit'"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "reviews=remove_punctuation(reviews)\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'this book be perfect im a first time new mom and this book make it so easy to keep track of fee diaper change sleep definitely would recommend this for new moms plus its small enough that i throw in the diaper back for doctor visit'"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "reviews=lemmatize_verbs(reviews)\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvectorizer=CountVectorizer()\n",
    "countvectorizer.fit(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'this': 317,\n 'book': 42,\n 'be': 32,\n 'perfect': 245,\n 'im': 152,\n 'first': 111,\n 'time': 321,\n 'new': 212,\n 'mom': 197,\n 'and': 21,\n 'make': 188,\n 'it': 163,\n 'so': 293,\n 'easy': 92,\n 'to': 324,\n 'keep': 168,\n 'track': 327,\n 'of': 220,\n 'fee': 106,\n 'diaper': 79,\n 'change': 53,\n 'sleep': 290,\n 'definitely': 76,\n 'would': 357,\n 'recommend': 266,\n 'for': 113,\n 'moms': 199,\n 'plus': 251,\n 'its': 164,\n 'small': 291,\n 'enough': 96,\n 'that': 308,\n 'throw': 319,\n 'in': 155,\n 'the': 310,\n 'back': 30,\n 'doctor': 84,\n 'visit': 338,\n 'use': 334,\n 'our': 233,\n 'babysitter': 29,\n 'grandma': 127,\n 'can': 49,\n 'what': 348,\n 'go': 125,\n 'on': 224,\n 'during': 87,\n 'day': 74,\n 'weve': 347,\n 'record': 267,\n 'many': 189,\n 'milestones': 196,\n 'since': 287,\n 'we': 343,\n 'receive': 265,\n 'month': 200,\n 'diary': 80,\n 'have': 135,\n 'room': 272,\n 'note': 217,\n 'wish': 354,\n 'hours': 144,\n 'start': 301,\n 'around': 24,\n 'until': 332,\n 'pm': 252,\n 'but': 46,\n 'just': 167,\n 'write': 358,\n 'over': 235,\n 'certain': 52,\n 'suit': 304,\n 'purpose': 259,\n 'nice': 213,\n 'thing': 314,\n 'if': 150,\n 'rough': 273,\n 'night': 214,\n 'super': 306,\n 'tire': 323,\n 'something': 295,\n 'refer': 268,\n 'forget': 114,\n 'when': 349,\n 'last': 171,\n 'baby': 28,\n 'or': 230,\n 'he': 136,\n 'wake': 339,\n 'up': 333,\n 'also': 19,\n 'like': 180,\n 'front': 118,\n 'page': 237,\n 'place': 248,\n 'emergency': 94,\n 'comment': 62,\n 'consent': 64,\n 'form': 115,\n 'case': 51,\n 'your': 362,\n 'child': 55,\n 'need': 211,\n 'medical': 193,\n 'care': 50,\n 'at': 27,\n 'immunizations': 153,\n 'ill': 151,\n 'get': 123,\n 'another': 22,\n 'one': 226,\n 'months': 201,\n 'finish': 110,\n 'log': 184,\n 'think': 316,\n 'work': 356,\n 'better': 37,\n 'with': 355,\n 'clearer': 58,\n 'section': 277,\n 'each': 88,\n '12': 0,\n 'you': 360,\n 'really': 264,\n 'two': 329,\n 'wet': 346,\n 'lot': 186,\n 'early': 90,\n 'morning': 203,\n 'between': 38,\n 'midnight': 195,\n '7am': 8,\n 'cram': 70,\n 'those': 318,\n 'blank': 41,\n 'space': 298,\n 'above': 11,\n 'right': 271,\n 'now': 219,\n 'my': 207,\n 'wife': 353,\n 'six': 288,\n 'old': 223,\n 'boy': 44,\n '4month': 5,\n 'mark': 190,\n 'decide': 75,\n 'she': 280,\n 'return': 270,\n 'instead': 160,\n 'stayathome': 302,\n 'hire': 141,\n 'an': 20,\n 'inhome': 158,\n 'nanny': 208,\n 'little': 182,\n 'arrangement': 25,\n 'quite': 261,\n 'well': 345,\n 'ever': 102,\n 'shortly': 284,\n 'after': 15,\n 'realize': 263,\n 'some': 294,\n 'sort': 297,\n 'journal': 165,\n 'boys': 45,\n 'activities': 13,\n 'while': 351,\n 'plain': 249,\n 'notebook': 218,\n 'period': 246,\n 'weeks': 344,\n 'stumble': 303,\n 'tracker': 328,\n 'daily': 72,\n 'childcare': 56,\n 'journallayout': 166,\n 'usethe': 336,\n 'excellent': 104,\n 'idea': 148,\n 'clearly': 59,\n 'divide': 82,\n 'into': 162,\n 'columns': 61,\n 'naptime': 210,\n 'playtime': 250,\n 'as': 26,\n 'general': 122,\n 'areas': 23,\n 'legibility': 177,\n 'huge': 147,\n 'improvement': 154,\n 'standard': 300,\n 'entries': 99,\n 'end': 95,\n 'become': 35,\n 'paragraph': 243,\n 'few': 107,\n 'short': 283,\n 'moments': 198,\n 'summarize': 305,\n 'all': 17,\n 'data': 73,\n 'total': 326,\n 'determine': 77,\n 'key': 169,\n 'information': 157,\n 'how': 145,\n 'much': 206,\n 'do': 83,\n 'eat': 93,\n 'bowel': 43,\n 'movement': 205,\n 'they': 312,\n 'etcthere': 101,\n 'however': 146,\n 'frustrate': 119,\n 'limitations': 181,\n 'entire': 97,\n 'layout': 173,\n 'about': 10,\n 'half': 130,\n 'sheet': 281,\n 'down': 86,\n 'middle': 194,\n 'portrait': 254,\n 'constrain': 66,\n 'very': 337,\n 'column': 60,\n 'row': 274,\n 'okay': 222,\n 'ounces': 232,\n 'by': 48,\n '341': 3,\n '234': 1,\n 'once': 225,\n 'more': 202,\n 'active': 12,\n 'youd': 361,\n 'know': 170,\n 'than': 307,\n '34tummy': 4,\n 'time34': 322,\n 'under': 330,\n 'things': 315,\n 'tight': 320,\n 'pageanother': 238,\n 'problem': 256,\n 'only': 228,\n 'cover': 69,\n 'out': 234,\n 'thats': 309,\n 'fine': 109,\n 'intention': 161,\n 'providers': 258,\n 'want': 340,\n 'which': 350,\n 'often': 221,\n 'earlier': 89,\n '6am': 6,\n 'require': 269,\n 'second': 276,\n 'pagewhat': 240,\n 'good': 126,\n 'read': 262,\n 'instantly': 159,\n 'gather': 121,\n 'matter': 191,\n 'high': 138,\n 'quality': 260,\n 'paper': 242,\n 'consistent': 65,\n 'pageswhat': 239,\n 'dont': 85,\n 'expensive': 105,\n 'most': 204,\n '6months': 7,\n 'less': 178,\n 'thick': 313,\n 'not': 216,\n 'hardback': 134,\n 'bend': 36,\n 'easily': 91,\n 'bag': 31,\n 'should': 285,\n 'bigger': 39,\n 'understand': 331,\n 'portability': 253,\n 'concern': 63,\n 'current': 71,\n 'size': 289,\n 'entirely': 98,\n 'too': 325,\n 'smallconclusioni': 292,\n 'own': 236,\n 'format': 116,\n 'spreadsheet': 299,\n 'include': 156,\n '24hour': 2,\n 'along': 18,\n 'other': 231,\n 'bind': 40,\n 'cheaply': 54,\n 'local': 183,\n 'shop': 282,\n 'adequate': 14,\n 'because': 34,\n 'son': 296,\n 'bear': 33,\n 'great': 129,\n 'handy': 132,\n 'way': 342,\n 'his': 142,\n 'discuss': 81,\n 'progress': 257,\n 'grandparents': 128,\n 'who': 352,\n 'watch': 341,\n 'him': 140,\n 'able': 9,\n 'maintain': 187,\n 'schedule': 275,\n 'me': 192,\n 'see': 278,\n 'highly': 139,\n 'parent': 244,\n 'children': 57,\n 'especially': 100,\n 'ones': 227,\n 'year': 359,\n 'age': 16,\n 'continue': 67,\n 'buy': 47,\n 'every': 103,\n 'least': 175,\n 'helpful': 137,\n 'nap': 209,\n 'developmental': 78,\n 'lay': 172,\n 'could': 68,\n 'leave': 176,\n 'open': 229,\n 'side': 286,\n 'set': 279,\n 'flip': 112,\n 'full': 120,\n 'nitpick': 215,\n 'let': 179,\n 'user': 335,\n 'fill': 108,\n 'then': 311,\n 'identical': 149,\n 'look': 185,\n 'glance': 124,\n 'hand': 131,\n 'hold': 143,\n 'pain': 241,\n 'lean': 174,\n 'forward': 117,\n 'pick': 247,\n 'happend': 133,\n 'prior': 255}"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "countvectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "363"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "len(countvectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(6, 363)"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "reviews_terms=countvectorizer.fit_transform(reviews)\n",
    "reviews_terms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<6x363 sparse matrix of type '<class 'numpy.int64'>'\n\twith 558 stored elements in Compressed Sparse Row format>"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "reviews_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(0, 317)\t3\n  (0, 42)\t2\n  (0, 32)\t1\n  (0, 245)\t1\n  (0, 152)\t1\n  (0, 111)\t1\n  (0, 321)\t1\n  (0, 212)\t2\n  (0, 197)\t1\n  (0, 21)\t1\n  (0, 188)\t1\n  (0, 163)\t1\n  (0, 293)\t1\n  (0, 92)\t1\n  (0, 324)\t1\n  (0, 168)\t1\n  (0, 327)\t1\n  (0, 220)\t1\n  (0, 106)\t1\n  (0, 79)\t2\n  (0, 53)\t1\n  (0, 290)\t1\n  (0, 76)\t1\n  (0, 357)\t1\n  (0, 266)\t1\n  :\t:\n  (5, 78)\t1\n  (5, 172)\t1\n  (5, 68)\t2\n  (5, 176)\t3\n  (5, 229)\t1\n  (5, 286)\t1\n  (5, 279)\t1\n  (5, 112)\t2\n  (5, 120)\t3\n  (5, 215)\t1\n  (5, 179)\t1\n  (5, 335)\t1\n  (5, 108)\t1\n  (5, 311)\t1\n  (5, 149)\t1\n  (5, 185)\t1\n  (5, 124)\t1\n  (5, 131)\t1\n  (5, 143)\t1\n  (5, 241)\t1\n  (5, 174)\t1\n  (5, 117)\t1\n  (5, 247)\t1\n  (5, 133)\t1\n  (5, 255)\t1\n"
    }
   ],
   "source": [
    "print(reviews_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0,\n        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n        0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        1, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "reviews_terms[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /home/syed/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n                ngram_range=(1, 1), preprocessor=None,\n                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n                            'itself', ...],\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "countvectorizer=CountVectorizer(stop_words=stopwords)\n",
    "countvectorizer.fit(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "290"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "len(countvectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\n                input='content', lowercase=True, max_df=1.0, max_features=None,\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n                smooth_idf=True,\n                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n                            'itself', ...],\n                strip_accents=None, sublinear_tf=False,\n                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n                vocabulary=None)"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvectorizer=TfidfVectorizer(stop_words=stopwords)\n",
    "tfidfvectorizer.fit(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          IDF Score\nhandy      2.252763\nhappend    2.252763\nhardback   2.252763\nfront      2.252763\nyoud       2.252763",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IDF Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>handy</th>\n      <td>2.252763</td>\n    </tr>\n    <tr>\n      <th>happend</th>\n      <td>2.252763</td>\n    </tr>\n    <tr>\n      <th>hardback</th>\n      <td>2.252763</td>\n    </tr>\n    <tr>\n      <th>front</th>\n      <td>2.252763</td>\n    </tr>\n    <tr>\n      <th>youd</th>\n      <td>2.252763</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "score=pd.DataFrame(tfidfvectorizer.idf_,index=tfidfvectorizer.get_feature_names(),columns=['IDF Score'])\n",
    "score.sort_values(by=['IDF Score']).tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<6x290 sparse matrix of type '<class 'numpy.float64'>'\n\twith 381 stored elements in Compressed Sparse Row format>"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "tfidfvectors=tfidfvectorizer.fit_transform(reviews)\n",
    "tfidfvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit34f49190624f45e4adb64b3571310d51"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}